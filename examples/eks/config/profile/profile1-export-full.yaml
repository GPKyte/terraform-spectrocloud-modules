name: herculus-cp NC_test
uid: 61e99a11b7e45ea188c87c3a
description: ""
type: cluster
cloudType: libvirt
packs:
  - name: centos-libvirt
    type: spectro
    layer: os
    registry_uid: 61b9a3180c4cf13de4695bbf
    registry: Public Repo
    version: "7.9"
    tag: "7.9"
    values: |-
      # Spectro Golden images includes most of the hardening standards recommended by CIS benchmarking v1.5
      
      # Uncomment below section to
      # 1. Include custom files to be copied over to the nodes and/or
      # 2. Execute list of commands before or after kubeadm init/join is executed
      #
      #kubeadmconfig:
      #  preKubeadmCommands:
      #  - echo "Executing pre kube admin config commands"
      #  - update-ca-certificates
      #  - 'systemctl restart containerd; sleep 3'
      #  - 'while [ ! -S /var/run/containerd/containerd.sock ]; do echo "Waiting for containerd..."; sleep 1; done'
      #  postKubeadmCommands:
      #  - echo "Executing post kube admin config commands"
      #  files:
      #  - targetPath: /usr/local/share/ca-certificates/mycom.crt
      #    targetOwner: "root:root"
      #    targetPermissions: "0644"
      #    content: |
      #      -----BEGIN CERTIFICATE-----
      #      MIICyzCCAbOgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
      #      cm5ldGVzMB4XDTIwMDkyMjIzNDMyM1oXDTMwMDkyMDIzNDgyM1owFTETMBEGA1UE
      #      AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMdA
      #      nZYs1el/6f9PgV/aO9mzy7MvqaZoFnqO7Qi4LZfYzixLYmMUzi+h8/RLPFIoYLiz
      #      qiDn+P8c9I1uxB6UqGrBt7dkXfjrUZPs0JXEOX9U/6GFXL5C+n3AUlAxNCS5jobN
      #      fbLt7DH3WoT6tLcQefTta2K+9S7zJKcIgLmBlPNDijwcQsbenSwDSlSLkGz8v6N2
      #      7SEYNCV542lbYwn42kbcEq2pzzAaCqa5uEPsR9y+uzUiJpv5tDHUdjbFT8tme3vL
      #      9EdCPODkqtMJtCvz0hqd5SxkfeC2L+ypaiHIxbwbWe7GtliROvz9bClIeGY7gFBK
      #      jZqpLdbBVjo0NZBTJFUCAwEAAaMmMCQwDgYDVR0PAQH/BAQDAgKkMBIGA1UdEwEB
      #      /wQIMAYBAf8CAQAwDQYJKoZIhvcNAQELBQADggEBADIKoE0P+aVJGV9LWGLiOhki
      #      HFv/vPPAQ2MPk02rLjWzCaNrXD7aPPgT/1uDMYMHD36u8rYyf4qPtB8S5REWBM/Y
      #      g8uhnpa/tGsaqO8LOFj6zsInKrsXSbE6YMY6+A8qvv5lPWpJfrcCVEo2zOj7WGoJ
      #      ixi4B3fFNI+wih8/+p4xW+n3fvgqVYHJ3zo8aRLXbXwztp00lXurXUyR8EZxyR+6
      #      b+IDLmHPEGsY9KOZ9VLLPcPhx5FR9njFyXvDKmjUMJJgUpRkmsuU1mCFC+OHhj56
      #      IkLaSJf6z/p2a3YjTxvHNCqFMLbJ2FvJwYCRzsoT2wm2oulnUAMWPI10vdVM+Nc=
      #      -----END CERTIFICATE-----
  - name: kubernetes-konvoy
    type: spectro
    layer: k8s
    registry_uid: 61b9a3180c4cf13de4695bbf
    registry: Public Repo
    version: 1.21.6
    tag: 1.21.6
    values: |-
      # spectrocloud.com/enabled-presets: Kube Controller Manager:loopback-ctrlmgr,Kube Scheduler:loopback-scheduler
      pack:
        k8sHardening: True
        #CIDR Range for Pods in cluster
        # Note : This must not overlap with any of the host or service network
        podCIDR: "192.168.0.0/16"
        #CIDR notation IP range from which to assign service cluster IPs
        # Note : This must not overlap with any IP ranges assigned to nodes for pods.
        serviceClusterIpRange: "10.96.0.0/12"
      
      # KubeAdm customization for kubernetes hardening. Below config will be ignored if k8sHardening property above is disabled
      kubeadmconfig:
        apiServer:
          extraArgs:
            # Note : secure-port flag is used during kubeadm init. Do not change this flag on a running cluster
            secure-port: "6443"
            anonymous-auth: "true"
            insecure-port: "0"
            profiling: "false"
            disable-admission-plugins: "AlwaysAdmit"
            default-not-ready-toleration-seconds: "60"
            default-unreachable-toleration-seconds: "60"
            enable-admission-plugins: "AlwaysPullImages,NamespaceLifecycle,ServiceAccount,NodeRestriction,PodSecurityPolicy"
            audit-log-path: /var/log/apiserver/audit.log
            audit-policy-file: /etc/kubernetes/audit-policy.yaml
            audit-log-maxage: "30"
            audit-log-maxbackup: "10"
            audit-log-maxsize: "100"
            authorization-mode: RBAC,Node
            tls-cipher-suites: "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256"
          extraVolumes:
            - name: audit-log
              hostPath: /var/log/apiserver
              mountPath: /var/log/apiserver
              pathType: DirectoryOrCreate
            - name: audit-policy
              hostPath: /etc/kubernetes/audit-policy.yaml
              mountPath: /etc/kubernetes/audit-policy.yaml
              readOnly: true
              pathType: File
        controllerManager:
          extraArgs:
            profiling: "false"
            terminated-pod-gc-threshold: "25"
            pod-eviction-timeout: "1m0s"
            use-service-account-credentials: "true"
            feature-gates: "RotateKubeletServerCertificate=true"
        scheduler:
          extraArgs:
            profiling: "false"
        kubeletExtraArgs:
          read-only-port : "0"
          event-qps: "0"
          feature-gates: "RotateKubeletServerCertificate=true"
          protect-kernel-defaults: "true"
          tls-cipher-suites: "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256"
        files:
          - path: hardening/audit-policy.yaml
            targetPath: /etc/kubernetes/audit-policy.yaml
            targetOwner: "root:root"
            targetPermissions: "0600"
          - path: hardening/privileged-psp.yaml
            targetPath: /etc/kubernetes/hardening/privileged-psp.yaml
            targetOwner: "root:root"
            targetPermissions: "0600"
          - path: hardening/90-kubelet.conf
            targetPath: /etc/sysctl.d/90-kubelet.conf
            targetOwner: "root:root"
            targetPermissions: "0600"
        preKubeadmCommands:
          # For enabling 'protect-kernel-defaults' flag to kubelet, kernel parameters changes are required
          - 'echo "====> Applying kernel parameters for Kubelet"'
          - 'sysctl -p /etc/sysctl.d/90-kubelet.conf'
        postKubeadmCommands:
          # Apply the privileged PodSecurityPolicy on the first master node ; Otherwise, CNI (and other) pods won't come up
          - 'export KUBECONFIG=/etc/kubernetes/admin.conf'
          # Sometimes api server takes a little longer to respond. Retry if applying the pod-security-policy manifest fails
          - '[ -f "$KUBECONFIG" ] && { echo " ====> Applying PodSecurityPolicy" ; until $(kubectl apply -f /etc/kubernetes/hardening/privileged-psp.yaml > /dev/null ); do echo "Failed to apply PodSecurityPolicies, will retry in 5s" ; sleep 5 ; done ; } || echo "Skipping PodSecurityPolicy for worker nodes"'
      
      # Client configuration to add OIDC based authentication flags in kubeconfig
      #clientConfig:
        #oidc-issuer-url: "{{ .spectro.pack.kubernetes.kubeadmconfig.apiServer.extraArgs.oidc-issuer-url }}"
        #oidc-client-id: "{{ .spectro.pack.kubernetes.kubeadmconfig.apiServer.extraArgs.oidc-client-id }}"
        #oidc-client-secret: 1gsranjjmdgahm10j8r6m47ejokm9kafvcbhi3d48jlc3rfpprhv
        #oidc-extra-scope: profile,email
  - name: cni-calico
    type: spectro
    layer: cni
    registry_uid: 61b9a3180c4cf13de4695bbf
    registry: Public Repo
    version: 3.19.0
    tag: 3.19.0
    values: |-
      manifests:
        calico:
      
          # IPAM type to use. Supported types are calico-ipam, host-local
          ipamType: "calico-ipam"
      
          # Should be one of CALICO_IPV4POOL_IPIP or CALICO_IPV4POOL_VXLAN
          encapsulationType: "CALICO_IPV4POOL_IPIP"
      
          # Should be one of Always, CrossSubnet, Never
          encapsulationMode: "Always"
  - name: csi-rook-ceph
    type: spectro
    layer: csi
    registry_uid: 61b9a3180c4cf13de4695bbf
    registry: Public Repo
    version: 1.8.0
    tag: 1.8.0
    values: |-
      # spectrocloud.com/enabled-presets: Storage - Block Devices:multi-node-block
      manifests:
        storageclass:
          contents: |
            apiVersion: ceph.rook.io/v1
            kind: CephFilesystem
            metadata:
              name: myfs
              namespace: rook-ceph # namespace:cluster
            spec:
              # The metadata pool spec. Must use replication.
              metadataPool:
                replicated:
                  size: 3
                  requireSafeReplicaSize: true
                parameters:
                  # Inline compression mode for the data pool
                  # Further reference: https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression
                  compression_mode:
                    none
                  # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
                  # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
                  #target_size_ratio: ".5"
              # The list of data pool specs. Can use replication or erasure coding.
              dataPools:
                - name: replicated
                  failureDomain: host
                  replicated:
                    size: 3
                    # Disallow setting pool with replica 1, this could lead to data loss without recovery.
                    # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
                    requireSafeReplicaSize: true
                  parameters:
                    # Inline compression mode for the data pool
                    # Further reference: https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression
                    compression_mode:
                      none
                    # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
                    # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
                    #target_size_ratio: ".5"
              # Whether to preserve filesystem after CephFilesystem CRD deletion
              preserveFilesystemOnDelete: true
              # The metadata service (mds) configuration
              metadataServer:
                # The number of active MDS instances
                activeCount: 1
                # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
                # If false, standbys will be available, but will not have a warm cache.
                activeStandby: true
                # The affinity rules to apply to the mds deployment
                placement:
                  #  nodeAffinity:
                  #    requiredDuringSchedulingIgnoredDuringExecution:
                  #      nodeSelectorTerms:
                  #      - matchExpressions:
                  #        - key: role
                  #          operator: In
                  #          values:
                  #          - mds-node
                  #  topologySpreadConstraints:
                  #  tolerations:
                  #  - key: mds-node
                  #    operator: Exists
                  #  podAffinity:
                  podAntiAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      - labelSelector:
                          matchExpressions:
                            - key: app
                              operator: In
                              values:
                                - rook-ceph-mds
                        # topologyKey: kubernetes.io/hostname will place MDS across different hosts
                        topologyKey: kubernetes.io/hostname
                    preferredDuringSchedulingIgnoredDuringExecution:
                      - weight: 100
                        podAffinityTerm:
                          labelSelector:
                            matchExpressions:
                              - key: app
                                operator: In
                                values:
                                  - rook-ceph-mds
                          # topologyKey: */zone can be used to spread MDS across different AZ
                          # Use <topologyKey: failure-domain.beta.kubernetes.io/zone> in k8s cluster if your cluster is v1.16 or lower
                          # Use <topologyKey: topology.kubernetes.io/zone>  in k8s cluster is v1.17 or upper
                          topologyKey: topology.kubernetes.io/zone
                # A key/value list of annotations
                annotations:
                #  key: value
                # A key/value list of labels
                labels:
                #  key: value
                resources:
                  # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory
                  #  limits:
                  #    cpu: "500m"
                  #    memory: "1024Mi"
                  #  requests:
                  #    cpu: "500m"
                  #    memory: "1024Mi"
                  # priorityClassName: my-priority-class
                  # Filesystem mirroring settings
                  # mirroring:
                  # enabled: true
                  # list of Kubernetes Secrets containing the peer token
                  # for more details see: https://docs.ceph.com/en/latest/dev/cephfs-mirroring/#bootstrap-peers
                  # peers:
                  #secretNames:
                  #- secondary-cluster-peer
                  # specify the schedule(s) on which snapshots should be taken
                  # see the official syntax here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-schedules
                  # snapshotSchedules:
                  #   - path: /
                  #     interval: 24h # daily snapshots
                  #     startTime: 11:55
                  # manage retention policies
                  # see syntax duration here https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-retention-policies
                  # snapshotRetention:
                  #   - path: /
                  #     duration: "h 24"
            ---
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: spectro-storage-class
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
            # Change "rook-ceph" provisioner prefix to match the operator namespace if needed
            provisioner: rook-ceph.cephfs.csi.ceph.com # driver:namespace:operator
            parameters:
              # clusterID is the namespace where the rook cluster is running
              # If you change this namespace, also change the namespace below where the secret namespaces are defined
              clusterID: rook-ceph # namespace:cluster
      
              # CephFS filesystem name into which the volume shall be created
              fsName: myfs
      
              # Ceph pool into which the volume shall be created
              # Required for provisionVolume: "true"
              pool: myfs-data0
      
              # The secrets contain Ceph admin credentials. These are generated automatically by the operator
              # in the same namespace as the cluster.
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
              csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster
      
              # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
              # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
              # or by setting the default mounter explicitly via --volumemounter command-line argument.
              # mounter: kernel
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            #Supported binding modes are Immediate, WaitForFirstConsumer
            volumeBindingMode: "WaitForFirstConsumer"
            mountOptions:
              # uncomment the following line for debugging
              #- debug
      
        cluster:
          contents: |
            apiVersion: ceph.rook.io/v1
            kind: CephCluster
            metadata:
              name: rook-ceph
              namespace: rook-ceph # namespace:cluster
            spec:
              cephVersion:
                # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
                # v15 is octopus, and v16 is pacific.
                # RECOMMENDATION: In production, use a specific version tag instead of the general v14 flag, which pulls the latest release and could result in different
                # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
                # If you want to be more precise, you can always use a timestamp tag such quay.io/ceph/ceph:v16.2.7-20211208
                # This tag might not contain a new Ceph version, just security fixes from the underlying operating system, which will reduce vulnerabilities
                image: quay.io/ceph/ceph:v16.2.7
                # Whether to allow unsupported versions of Ceph. Currently `octopus` and `pacific` are supported.
                # Future versions such as `pacific` would require this to be set to `true`.
                # Do not set to true in production.
                allowUnsupported: false
              # The path on the host where configuration files will be persisted. Must be specified.
              # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
              # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
              dataDirHostPath: /var/lib/rook
              # Whether or not upgrade should continue even if a check fails
              # This means Ceph's status could be degraded and we don't recommend upgrading but you might decide otherwise
              # Use at your OWN risk
              # To understand Rook's upgrade process of Ceph, read https://rook.io/docs/rook/latest/ceph-upgrade.html#ceph-version-upgrades
              skipUpgradeChecks: false
              # Whether or not continue if PGs are not clean during an upgrade
              continueUpgradeAfterChecksEvenIfNotHealthy: false
              # WaitTimeoutForHealthyOSDInMinutes defines the time (in minutes) the operator would wait before an OSD can be stopped for upgrade or restart.
              # If the timeout exceeds and OSD is not ok to stop, then the operator would skip upgrade for the current OSD and proceed with the next one
              # if `continueUpgradeAfterChecksEvenIfNotHealthy` is `false`. If `continueUpgradeAfterChecksEvenIfNotHealthy` is `true`, then opertor would
              # continue with the upgrade of an OSD even if its not ok to stop after the timeout. This timeout won't be applied if `skipUpgradeChecks` is `true`.
              # The default wait timeout is 10 minutes.
              waitTimeoutForHealthyOSDInMinutes: 10
              mon:
                # Set the number of mons to be started. Generally recommended to be 3.
                # For highest availability, an odd number of mons should be specified.
                count: 3
                # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
                # Mons should only be allowed on the same node for test environments where data loss is acceptable.
                allowMultiplePerNode: false
              mgr:
                # When higher availability of the mgr is needed, increase the count to 2.
                # In that case, one mgr will be active and one in standby. When Ceph updates which
                # mgr is active, Rook will update the mgr services to match the active mgr.
                count: 1
                modules:
                  # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
                  # are already enabled by other settings in the cluster CR.
                  - name: pg_autoscaler
                    enabled: true
              # enable the ceph dashboard for viewing cluster status
              dashboard:
                enabled: true
                # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
                # urlPrefix: /ceph-dashboard
                # serve the dashboard at the given port.
                # port: 8443
                # serve the dashboard using SSL
                ssl: true
              # enable prometheus alerting for cluster
              monitoring:
                # requires Prometheus to be pre-installed
                enabled: false
                # namespace to deploy prometheusRule in. If empty, namespace of the cluster will be used.
                # Recommended:
                # If you have a single rook-ceph cluster, set the rulesNamespace to the same namespace as the cluster or keep it empty.
                # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
                # deployed) to set rulesNamespace for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
                rulesNamespace: rook-ceph
              network:
                # enable host networking
                #provider: host
                # enable the Multus network provider
                #provider: multus
                #selectors:
                  # The selector keys are required to be `public` and `cluster`.
                  # Based on the configuration, the operator will do the following:
                  #   1. if only the `public` selector key is specified both public_network and cluster_network Ceph settings will listen on that interface
                  #   2. if both `public` and `cluster` selector keys are specified the first one will point to 'public_network' flag and the second one to 'cluster_network'
                  #
                  # In order to work, each selector value must match a NetworkAttachmentDefinition object in Multus
                  #
                  #public: public-conf --> NetworkAttachmentDefinition object name in Multus
                  #cluster: cluster-conf --> NetworkAttachmentDefinition object name in Multus
                # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
                #ipFamily: "IPv6"
                # Ceph daemons to listen on both IPv4 and Ipv6 networks
                #dualStack: false
              # enable the crash collector for ceph daemon crash collection
              crashCollector:
                disable: false
                # Uncomment daysToRetain to prune ceph crash entries older than the
                # specified number of days.
                #daysToRetain: 30
              # enable log collector, daemons will log on files and rotate
              # logCollector:
              #   enabled: true
              #   periodicity: 24h # SUFFIX may be 'h' for hours or 'd' for days.
              # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
              cleanupPolicy:
                # Since cluster cleanup is destructive to data, confirmation is required.
                # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
                # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
                # Rook will immediately stop configuring the cluster and only wait for the delete command.
                # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
                confirmation: ""
                # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
                sanitizeDisks:
                  # method indicates if the entire disk should be sanitized or simply ceph's metadata
                  # in both case, re-install is possible
                  # possible choices are 'complete' or 'quick' (default)
                  method: quick
                  # dataSource indicate where to get random bytes from to write on the disk
                  # possible choices are 'zero' (default) or 'random'
                  # using random sources will consume entropy from the system and will take much more time then the zero source
                  dataSource: zero
                  # iteration overwrite N times instead of the default (1)
                  # takes an integer value
                  iteration: 1
                # allowUninstallWithVolumes defines how the uninstall should be performed
                # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
                allowUninstallWithVolumes: false
              # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
              # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
              # tolerate taints with a key of 'storage-node'.
            #  placement:
            #    all:
            #      nodeAffinity:
            #        requiredDuringSchedulingIgnoredDuringExecution:
            #          nodeSelectorTerms:
            #          - matchExpressions:
            #            - key: role
            #              operator: In
            #              values:
            #              - storage-node
            #      podAffinity:
            #      podAntiAffinity:
            #      topologySpreadConstraints:
            #      tolerations:
            #      - key: storage-node
            #        operator: Exists
            # The above placement information can also be specified for mon, osd, and mgr components
            #    mon:
            # Monitor deployments may contain an anti-affinity rule for avoiding monitor
            # collocation on the same node. This is a required rule when host network is used
            # or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
            # preferred rule with weight: 50.
            #    osd:
            #    mgr:
            #    cleanup:
              annotations:
            #    all:
            #    mon:
            #    osd:
            #    cleanup:
            #    prepareosd:
            # If no mgr annotations are set, prometheus scrape annotations will be set by default.
            #    mgr:
              labels:
            #    all:
            #    mon:
            #    osd:
            #    cleanup:
            #    mgr:
            #    prepareosd:
            # monitoring is a list of key-value pairs. It is injected into all the monitoring resources created by operator.
            # These labels can be passed as LabelSelector to Prometheus
            #    monitoring:
            #    crashcollector:
              resources:
            # The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
            #    mgr:
            #      limits:
            #        cpu: "500m"
            #        memory: "1024Mi"
            #      requests:
            #        cpu: "500m"
            #        memory: "1024Mi"
            # The above example requests/limits can also be added to the other components
            #    mon:
            #    osd:
            # For OSD it also is a possible to specify requests/limits based on device class
            #    osd-hdd:
            #    osd-ssd:
            #    osd-nvme:
            #    prepareosd:
            #    mgr-sidecar:
            #    crashcollector:
            #    logcollector:
            #    cleanup:
              # The option to automatically remove OSDs that are out and are safe to destroy.
              removeOSDsIfOutAndSafeToRemove: true
            #  priorityClassNames:
            #    all: rook-ceph-default-priority-class
            #    mon: rook-ceph-mon-priority-class
            #    osd: rook-ceph-osd-priority-class
            #    mgr: rook-ceph-mgr-priority-class
              storage: # cluster level storage configuration and selection
                useAllNodes: true
                useAllDevices: true
                #deviceFilter:
                config:
                  # crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
                  # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
                  # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
                  # journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
                  # osdsPerDevice: "1" # this value can be overridden at the node or device level
                  # encryptedDevice: "true" # the default value for this option is "false"
            # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
            # nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
                # nodes:
                #   - name: "172.17.4.201"
                #     devices: # specific devices to use for storage can be specified for each node
                #       - name: "sdb"
                #       - name: "nvme01" # multiple osds can be created on high performance devices
                #         config:
                #           osdsPerDevice: "5"
                #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
                #     config: # configuration can be specified at the node level which overrides the cluster level config
                #   - name: "172.17.4.301"
                #     deviceFilter: "^sd."
                # when onlyApplyOSDPlacement is false, will merge both placement.All() and placement.osd
                onlyApplyOSDPlacement: false
              # The section for configuring management of daemon disruptions during upgrade or fencing.
              disruptionManagement:
                # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
                # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
                # block eviction of OSDs by default and unblock them safely when drains are detected.
                managePodBudgets: true
                # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
                # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
                osdMaintenanceTimeout: 30
                # A duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and OSDs came back up.
                # Operator will continue with the next drain if the timeout exceeds. It only works if `managePodBudgets` is `true`.
                # No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
                pgHealthCheckTimeout: 0
                # If true, the operator will create and manage MachineDisruptionBudgets to ensure OSDs are only fenced when the cluster is healthy.
                # Only available on OpenShift.
                manageMachineDisruptionBudgets: false
                # Namespace in which to watch for the MachineDisruptionBudgets.
                machineDisruptionBudgetNamespace: openshift-machine-api
      
              # healthChecks
              # Valid values for daemons are 'mon', 'osd', 'status'
              healthCheck:
                daemonHealth:
                  mon:
                    disabled: false
                    interval: 45s
                  osd:
                    disabled: false
                    interval: 60s
                  status:
                    disabled: false
                    interval: 60s
                # Change pod liveness probe, it works for all mon,mgr,osd daemons
                livenessProbe:
                  mon:
                    disabled: false
                  mgr:
                    disabled: false
                  osd:
                    disabled: false
      
        operator:
          contents: |
      
            # Rook Ceph Operator Config ConfigMap
            # Use this ConfigMap to override Rook-Ceph Operator configurations.
            # NOTE! Precedence will be given to this config if the same Env Var config also exists in the
            #       Operator Deployment.
            # To move a configuration(s) from the Operator Deployment to this ConfigMap, add the config
            # here. It is recommended to then remove it from the Deployment to eliminate any future confusion.
      
            kind: ConfigMap
            apiVersion: v1
            metadata:
              name: rook-ceph-operator-config
              # should be in the namespace of the operator
              namespace: rook-ceph # namespace:operator
            data:
              # The logging level for the operator: ERROR | WARNING | INFO | DEBUG
              ROOK_LOG_LEVEL: "INFO"
      
              # Enable the CSI driver.
              # To run the non-default version of the CSI driver, see the override-able image properties in operator.yaml
              ROOK_CSI_ENABLE_CEPHFS: "true"
              # Enable the default version of the CSI RBD driver. To start another version of the CSI driver, see image properties below.
              ROOK_CSI_ENABLE_RBD: "true"
              ROOK_CSI_ENABLE_GRPC_METRICS: "false"
      
              # Set to true to enable host networking for CSI CephFS and RBD nodeplugins. This may be necessary
              # in some network configurations where the SDN does not provide access to an external cluster or
              # there is significant drop in read/write performance.
              # CSI_ENABLE_HOST_NETWORK: "true"
      
              # Set logging level for csi containers.
              # Supported values from 0 to 5. 0 for general useful logs, 5 for trace level verbosity.
              # CSI_LOG_LEVEL: "0"
      
              # Set replicas for csi provisioner deployment.
              CSI_PROVISIONER_REPLICAS: "2"
      
              # OMAP generator will generate the omap mapping between the PV name and the RBD image.
              # CSI_ENABLE_OMAP_GENERATOR need to be enabled when we are using rbd mirroring feature.
              # By default OMAP generator sidecar is deployed with CSI provisioner pod, to disable
              # it set it to false.
              # CSI_ENABLE_OMAP_GENERATOR: "false"
      
              # set to false to disable deployment of snapshotter container in CephFS provisioner pod.
              CSI_ENABLE_CEPHFS_SNAPSHOTTER: "true"
      
              # set to false to disable deployment of snapshotter container in RBD provisioner pod.
              CSI_ENABLE_RBD_SNAPSHOTTER: "true"
      
              # Enable cephfs kernel driver instead of ceph-fuse.
              # If you disable the kernel client, your application may be disrupted during upgrade.
              # See the upgrade guide: https://rook.io/docs/rook/latest/ceph-upgrade.html
              # NOTE! cephfs quota is not supported in kernel version < 4.17
              CSI_FORCE_CEPHFS_KERNEL_CLIENT: "true"
      
              # (Optional) policy for modifying a volume's ownership or permissions when the RBD PVC is being mounted.
              # supported values are documented at https://kubernetes-csi.github.io/docs/support-fsgroup.html
              CSI_RBD_FSGROUPPOLICY: "ReadWriteOnceWithFSType"
      
              # (Optional) policy for modifying a volume's ownership or permissions when the CephFS PVC is being mounted.
              # supported values are documented at https://kubernetes-csi.github.io/docs/support-fsgroup.html
              CSI_CEPHFS_FSGROUPPOLICY: "None"
      
              # (Optional) Allow starting unsupported ceph-csi image
              ROOK_CSI_ALLOW_UNSUPPORTED_VERSION: "false"
      
              # (Optional) control the host mount of /etc/selinux for csi plugin pods.
              CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT: "false"
      
              # The default version of CSI supported by Rook will be started. To change the version
              # of the CSI driver to something other than what is officially supported, change
              # these images to the desired release of the CSI driver.
              # ROOK_CSI_CEPH_IMAGE: "quay.io/cephcsi/cephcsi:v3.4.0"
              # ROOK_CSI_REGISTRAR_IMAGE: "k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0"
              # ROOK_CSI_RESIZER_IMAGE: "k8s.gcr.io/sig-storage/csi-resizer:v1.3.0"
              # ROOK_CSI_PROVISIONER_IMAGE: "k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0"
              # ROOK_CSI_SNAPSHOTTER_IMAGE: "k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0"
              # ROOK_CSI_ATTACHER_IMAGE: "k8s.gcr.io/sig-storage/csi-attacher:v3.3.0"
      
              # (Optional) set user created priorityclassName for csi plugin pods.
              # CSI_PLUGIN_PRIORITY_CLASSNAME: "system-node-critical"
      
              # (Optional) set user created priorityclassName for csi provisioner pods.
              # CSI_PROVISIONER_PRIORITY_CLASSNAME: "system-cluster-critical"
      
              # CSI CephFS plugin daemonset update strategy, supported values are OnDelete and RollingUpdate.
              # Default value is RollingUpdate.
              # CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY: "OnDelete"
              # CSI RBD plugin daemonset update strategy, supported values are OnDelete and RollingUpdate.
              # Default value is RollingUpdate.
              # CSI_RBD_PLUGIN_UPDATE_STRATEGY: "OnDelete"
      
              # kubelet directory path, if kubelet configured to use other than /var/lib/kubelet path.
              # ROOK_CSI_KUBELET_DIR_PATH: "/var/lib/kubelet"
      
              # Labels to add to the CSI CephFS Deployments and DaemonSets Pods.
              # ROOK_CSI_CEPHFS_POD_LABELS: "key1=value1,key2=value2"
              # Labels to add to the CSI RBD Deployments and DaemonSets Pods.
              # ROOK_CSI_RBD_POD_LABELS: "key1=value1,key2=value2"
      
              # (Optional) CephCSI provisioner NodeAffinity(applied to both CephFS and RBD provisioner).
              # CSI_PROVISIONER_NODE_AFFINITY: "role=storage-node; storage=rook, ceph"
              # (Optional) CephCSI provisioner tolerations list(applied to both CephFS and RBD provisioner).
              # Put here list of taints you want to tolerate in YAML format.
              # CSI provisioner would be best to start on the same nodes as other ceph daemons.
              # CSI_PROVISIONER_TOLERATIONS: |
              #   - effect: NoSchedule
              #     key: node-role.kubernetes.io/controlplane
              #     operator: Exists
              #   - effect: NoExecute
              #     key: node-role.kubernetes.io/etcd
              #     operator: Exists
              # (Optional) CephCSI plugin NodeAffinity(applied to both CephFS and RBD plugin).
              # CSI_PLUGIN_NODE_AFFINITY: "role=storage-node; storage=rook, ceph"
              # (Optional) CephCSI plugin tolerations list(applied to both CephFS and RBD plugin).
              # Put here list of taints you want to tolerate in YAML format.
              # CSI plugins need to be started on all the nodes where the clients need to mount the storage.
              # CSI_PLUGIN_TOLERATIONS: |
              #   - effect: NoSchedule
              #     key: node-role.kubernetes.io/controlplane
              #     operator: Exists
              #   - effect: NoExecute
              #     key: node-role.kubernetes.io/etcd
              #     operator: Exists
      
              # (Optional) CephCSI RBD provisioner NodeAffinity(if specified, overrides CSI_PROVISIONER_NODE_AFFINITY).
              # CSI_RBD_PROVISIONER_NODE_AFFINITY: "role=rbd-node"
              # (Optional) CephCSI RBD provisioner tolerations list(if specified, overrides CSI_PROVISIONER_TOLERATIONS).
              # Put here list of taints you want to tolerate in YAML format.
              # CSI provisioner would be best to start on the same nodes as other ceph daemons.
              # CSI_RBD_PROVISIONER_TOLERATIONS: |
              #   - key: node.rook.io/rbd
              #     operator: Exists
              # (Optional) CephCSI RBD plugin NodeAffinity(if specified, overrides CSI_PLUGIN_NODE_AFFINITY).
              # CSI_RBD_PLUGIN_NODE_AFFINITY: "role=rbd-node"
              # (Optional) CephCSI RBD plugin tolerations list(if specified, overrides CSI_PLUGIN_TOLERATIONS).
              # Put here list of taints you want to tolerate in YAML format.
              # CSI plugins need to be started on all the nodes where the clients need to mount the storage.
              # CSI_RBD_PLUGIN_TOLERATIONS: |
              #   - key: node.rook.io/rbd
              #     operator: Exists
      
              # (Optional) CephCSI CephFS provisioner NodeAffinity(if specified, overrides CSI_PROVISIONER_NODE_AFFINITY).
              # CSI_CEPHFS_PROVISIONER_NODE_AFFINITY: "role=cephfs-node"
              # (Optional) CephCSI CephFS provisioner tolerations list(if specified, overrides CSI_PROVISIONER_TOLERATIONS).
              # Put here list of taints you want to tolerate in YAML format.
              # CSI provisioner would be best to start on the same nodes as other ceph daemons.
              # CSI_CEPHFS_PROVISIONER_TOLERATIONS: |
              #   - key: node.rook.io/cephfs
              #     operator: Exists
              # (Optional) CephCSI CephFS plugin NodeAffinity(if specified, overrides CSI_PLUGIN_NODE_AFFINITY).
              # CSI_CEPHFS_PLUGIN_NODE_AFFINITY: "role=cephfs-node"
              # (Optional) CephCSI CephFS plugin tolerations list(if specified, overrides CSI_PLUGIN_TOLERATIONS).
              # Put here list of taints you want to tolerate in YAML format.
              # CSI plugins need to be started on all the nodes where the clients need to mount the storage.
              # CSI_CEPHFS_PLUGIN_TOLERATIONS: |
              #   - key: node.rook.io/cephfs
              #     operator: Exists
      
              # (Optional) CEPH CSI RBD provisioner resource requirement list, Put here list of resource
              # requests and limits you want to apply for provisioner pod
              # CSI_RBD_PROVISIONER_RESOURCE: |
              #  - name : csi-provisioner
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 100m
              #      limits:
              #        memory: 256Mi
              #        cpu: 200m
              #  - name : csi-resizer
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 100m
              #      limits:
              #        memory: 256Mi
              #        cpu: 200m
              #  - name : csi-attacher
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 100m
              #      limits:
              #        memory: 256Mi
              #        cpu: 200m
              #  - name : csi-snapshotter
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 100m
              #      limits:
              #        memory: 256Mi
              #        cpu: 200m
              #  - name : csi-rbdplugin
              #    resource:
              #      requests:
              #        memory: 512Mi
              #        cpu: 250m
              #      limits:
              #        memory: 1Gi
              #        cpu: 500m
              #  - name : liveness-prometheus
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 50m
              #      limits:
              #        memory: 256Mi
              #        cpu: 100m
              # (Optional) CEPH CSI RBD plugin resource requirement list, Put here list of resource
              # requests and limits you want to apply for plugin pod
              # CSI_RBD_PLUGIN_RESOURCE: |
              #  - name : driver-registrar
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 50m
              #      limits:
              #        memory: 256Mi
              #        cpu: 100m
              #  - name : csi-rbdplugin
              #    resource:
              #      requests:
              #        memory: 512Mi
              #        cpu: 250m
              #      limits:
              #        memory: 1Gi
              #        cpu: 500m
              #  - name : liveness-prometheus
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 50m
              #      limits:
              #        memory: 256Mi
              #        cpu: 100m
              # (Optional) CEPH CSI CephFS provisioner resource requirement list, Put here list of resource
              # requests and limits you want to apply for provisioner pod
              # CSI_CEPHFS_PROVISIONER_RESOURCE: |
              #  - name : csi-provisioner
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 100m
              #      limits:
              #        memory: 256Mi
              #        cpu: 200m
              #  - name : csi-resizer
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 100m
              #      limits:
              #        memory: 256Mi
              #        cpu: 200m
              #  - name : csi-attacher
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 100m
              #      limits:
              #        memory: 256Mi
              #        cpu: 200m
              #  - name : csi-cephfsplugin
              #    resource:
              #      requests:
              #        memory: 512Mi
              #        cpu: 250m
              #      limits:
              #        memory: 1Gi
              #        cpu: 500m
              #  - name : liveness-prometheus
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 50m
              #      limits:
              #        memory: 256Mi
              #        cpu: 100m
              # (Optional) CEPH CSI CephFS plugin resource requirement list, Put here list of resource
              # requests and limits you want to apply for plugin pod
              # CSI_CEPHFS_PLUGIN_RESOURCE: |
              #  - name : driver-registrar
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 50m
              #      limits:
              #        memory: 256Mi
              #        cpu: 100m
              #  - name : csi-cephfsplugin
              #    resource:
              #      requests:
              #        memory: 512Mi
              #        cpu: 250m
              #      limits:
              #        memory: 1Gi
              #        cpu: 500m
              #  - name : liveness-prometheus
              #    resource:
              #      requests:
              #        memory: 128Mi
              #        cpu: 50m
              #      limits:
              #        memory: 256Mi
              #        cpu: 100m
      
              # Configure CSI CSI Ceph FS grpc and liveness metrics port
              # CSI_CEPHFS_GRPC_METRICS_PORT: "9091"
              # CSI_CEPHFS_LIVENESS_METRICS_PORT: "9081"
              # Configure CSI RBD grpc and liveness metrics port
              # CSI_RBD_GRPC_METRICS_PORT: "9090"
              # CSI_RBD_LIVENESS_METRICS_PORT: "9080"
      
              # Whether the OBC provisioner should watch on the operator namespace or not, if not the namespace of the cluster will be used
              ROOK_OBC_WATCH_OPERATOR_NAMESPACE: "true"
      
              # Whether to start the discovery daemon to watch for raw storage devices on nodes in the cluster.
              # This daemon does not need to run if you are only going to create your OSDs based on StorageClassDeviceSets with PVCs.
              ROOK_ENABLE_DISCOVERY_DAEMON: "false"
              # The timeout value (in seconds) of Ceph commands. It should be >= 1. If this variable is not set or is an invalid value, it's default to 15.
              ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS: "15"
              # Enable the volume replication controller.
              # Before enabling, ensure the Volume Replication CRDs are created.
              # See https://rook.io/docs/rook/latest/ceph-csi-drivers.html#rbd-mirroring
              CSI_ENABLE_VOLUME_REPLICATION: "false"
              # CSI_VOLUME_REPLICATION_IMAGE: "quay.io/csiaddons/volumereplication-operator:v0.1.0"
            ---
            # OLM: BEGIN OPERATOR DEPLOYMENT
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: rook-ceph-operator
              namespace: rook-ceph # namespace:operator
              labels:
                operator: rook
                storage-backend: ceph
                app.kubernetes.io/name: rook-ceph
                app.kubernetes.io/instance: rook-ceph
                app.kubernetes.io/component: rook-ceph-operator
                app.kubernetes.io/part-of: rook-ceph-operator
            spec:
              selector:
                matchLabels:
                  app: rook-ceph-operator
              replicas: 1
              template:
                metadata:
                  labels:
                    app: rook-ceph-operator
                spec:
                  serviceAccountName: rook-ceph-system
                  containers:
                    - name: rook-ceph-operator
                      image: rook/ceph:v1.8.0
                      args: ["ceph", "operator"]
                      securityContext:
                        runAsNonRoot: true
                        runAsUser: 2016
                        runAsGroup: 2016
                      volumeMounts:
                        - mountPath: /var/lib/rook
                          name: rook-config
                        - mountPath: /etc/ceph
                          name: default-config-dir
                        - mountPath: /etc/webhook
                          name: webhook-cert
                      ports:
                        - containerPort: 9443
                          name: https-webhook
                          protocol: TCP
                      env:
                        # If the operator should only watch for cluster CRDs in the same namespace, set this to "true".
                        # If this is not set to true, the operator will watch for cluster CRDs in all namespaces.
                        - name: ROOK_CURRENT_NAMESPACE_ONLY
                          value: "false"
                        # Rook Discover toleration. Will tolerate all taints with all keys.
                        # Choose between NoSchedule, PreferNoSchedule and NoExecute:
                        # - name: DISCOVER_TOLERATION
                        #   value: "NoSchedule"
                        # (Optional) Rook Discover toleration key. Set this to the key of the taint you want to tolerate
                        # - name: DISCOVER_TOLERATION_KEY
                        #   value: "<KeyOfTheTaintToTolerate>"
                        # (Optional) Rook Discover tolerations list. Put here list of taints you want to tolerate in YAML format.
                        # - name: DISCOVER_TOLERATIONS
                        #   value: |
                        #     - effect: NoSchedule
                        #       key: node-role.kubernetes.io/controlplane
                        #       operator: Exists
                        #     - effect: NoExecute
                        #       key: node-role.kubernetes.io/etcd
                        #       operator: Exists
                        # (Optional) Rook Discover priority class name to set on the pod(s)
                        # - name: DISCOVER_PRIORITY_CLASS_NAME
                        #   value: "<PriorityClassName>"
                        # (Optional) Discover Agent NodeAffinity.
                        # - name: DISCOVER_AGENT_NODE_AFFINITY
                        #   value: "role=storage-node; storage=rook, ceph"
                        # (Optional) Discover Agent Pod Labels.
                        # - name: DISCOVER_AGENT_POD_LABELS
                        #   value: "key1=value1,key2=value2"
      
                        # The duration between discovering devices in the rook-discover daemonset.
                        - name: ROOK_DISCOVER_DEVICES_INTERVAL
                          value: "60m"
      
                        # Whether to start pods as privileged that mount a host path, which includes the Ceph mon and osd pods.
                        # Set this to true if SELinux is enabled (e.g. OpenShift) to workaround the anyuid issues.
                        # For more details see https://github.com/rook/rook/issues/1314#issuecomment-355799641
                        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
                          value: "false"
      
                        # In some situations SELinux relabelling breaks (times out) on large filesystems, and doesn't work with cephfs ReadWriteMany volumes (last relabel wins).
                        # Disable it here if you have similar issues.
                        # For more details see https://github.com/rook/rook/issues/2417
                        - name: ROOK_ENABLE_SELINUX_RELABELING
                          value: "true"
      
                        # In large volumes it will take some time to chown all the files. Disable it here if you have performance issues.
                        # For more details see https://github.com/rook/rook/issues/2254
                        - name: ROOK_ENABLE_FSGROUP
                          value: "true"
      
                        # Disable automatic orchestration when new devices are discovered
                        - name: ROOK_DISABLE_DEVICE_HOTPLUG
                          value: "false"
      
                        # Provide customised regex as the values using comma. For eg. regex for rbd based volume, value will be like "(?i)rbd[0-9]+".
                        # In case of more than one regex, use comma to separate between them.
                        # Default regex will be "(?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+"
                        # Add regex expression after putting a comma to blacklist a disk
                        # If value is empty, the default regex will be used.
                        - name: DISCOVER_DAEMON_UDEV_BLACKLIST
                          value: "(?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+"
      
                        # Time to wait until the node controller will move Rook pods to other
                        # nodes after detecting an unreachable node.
                        # Pods affected by this setting are:
                        # mgr, rbd, mds, rgw, nfs, PVC based mons and osds, and ceph toolbox
                        # The value used in this variable replaces the default value of 300 secs
                        # added automatically by k8s as Toleration for
                        # <node.kubernetes.io/unreachable>
                        # The total amount of time to reschedule Rook pods in healthy nodes
                        # before detecting a <not ready node> condition will be the sum of:
                        #  --> node-monitor-grace-period: 40 seconds (k8s kube-controller-manager flag)
                        #  --> ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS: 5 seconds
                        - name: ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS
                          value: "5"
      
                        # The name of the node to pass with the downward API
                        - name: NODE_NAME
                          valueFrom:
                            fieldRef:
                              fieldPath: spec.nodeName
                        # The pod name to pass with the downward API
                        - name: POD_NAME
                          valueFrom:
                            fieldRef:
                              fieldPath: metadata.name
                        # The pod namespace to pass with the downward API
                        - name: POD_NAMESPACE
                          valueFrom:
                            fieldRef:
                              fieldPath: metadata.namespace
                      # Recommended resource requests and limits, if desired
                      #resources:
                      #  limits:
                      #    cpu: 500m
                      #    memory: 256Mi
                      #  requests:
                      #    cpu: 100m
                      #    memory: 128Mi
      
                      #  Uncomment it to run lib bucket provisioner in multithreaded mode
                      #- name: LIB_BUCKET_PROVISIONER_THREADS
                      #  value: "5"
      
                  # Uncomment it to run rook operator on the host network
                  #hostNetwork: true
                  volumes:
                    - name: rook-config
                      emptyDir: {}
                    - name: default-config-dir
                      emptyDir: {}
                    - name: webhook-cert
                      emptyDir: {}
            # OLM: END OPERATOR DEPLOYMENT
      
        toolbox:
          contents: |
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: rook-ceph-tools
              namespace: rook-ceph # namespace:cluster
              labels:
                app: rook-ceph-tools
            spec:
              replicas: 1
              selector:
                matchLabels:
                  app: rook-ceph-tools
              template:
                metadata:
                  labels:
                    app: rook-ceph-tools
                spec:
                  dnsPolicy: ClusterFirstWithHostNet
                  containers:
                    - name: rook-ceph-tools
                      image: rook/ceph:v1.8.4
                      command: ["/bin/bash"]
                      args: ["-m", "-c", "/usr/local/bin/toolbox.sh"]
                      imagePullPolicy: IfNotPresent
                      tty: true
                      securityContext:
                        runAsNonRoot: true
                        runAsUser: 2016
                        runAsGroup: 2016
                      env:
                        - name: ROOK_CEPH_USERNAME
                          valueFrom:
                            secretKeyRef:
                              name: rook-ceph-mon
                              key: ceph-username
                        - name: ROOK_CEPH_SECRET
                          valueFrom:
                            secretKeyRef:
                              name: rook-ceph-mon
                              key: ceph-secret
                      volumeMounts:
                        - mountPath: /etc/ceph
                          name: ceph-config
                        - name: mon-endpoint-volume
                          mountPath: /etc/rook
                  volumes:
                    - name: mon-endpoint-volume
                      configMap:
                        name: rook-ceph-mon-endpoints
                        items:
                          - key: data
                            path: mon-endpoints
                    - name: ceph-config
                      emptyDir: {}
                  tolerations:
                    - key: "node.kubernetes.io/unreachable"
                      operator: "Exists"
                      effect: "NoExecute"
                      tolerationSeconds: 5